# Story 1.3: 知識庫數據模型與導入工具（詳細版）
> **🔴 MVP Priority: Phase 1** - 核心價值，AI搜索的基礎
> **⏱️ 預估工作量**: 12-15天
> **👥 需要角色**: 後端開發者, AI工程師

## User Story
作為一名系統管理員，
我想要批量導入各種格式的產品文檔和銷售資料到知識庫，
以便銷售團隊能夠查詢最新、最準確的資訊。

## 背景說明
知識庫是整個系統的核心資產。企業通常有大量現存的文檔散落在不同系統中（SharePoint、檔案伺服器、CRM）。我們需要一個強大的導入系統，能夠處理各種格式，提取結構化資訊，並建立可搜索的索引。向量化處理是實現語義搜索的關鍵。

## 技術規格
- **文檔解析**：Apache Tika 或 Unstructured.io
- **向量化模型**：OpenAI text-embedding-3-small 或 Sentence Transformers
- **向量維度**：1536 維（根據選擇的模型）
- **批次處理**：使用 Bull Queue 處理大量文檔
- **儲存策略**：原始文檔在對象儲存，元數據和向量在 PostgreSQL

## 驗收標準

### 1. 數據模型設計：
- 文檔表：ID、標題、類型、來源、版本、狀態
- 內容分塊表：支援長文檔分段處理
- 標籤系統：多對多關係，支援階層式標籤
- 版本控制：保留歷史版本，支援 diff 查看
- 權限控制：文檔級別的存取控制

### 2. 文檔解析能力：
- PDF：包含文字提取、表格識別、圖片 OCR
- Office 文檔：保留格式資訊（標題、列表、表格）
- HTML/Markdown：保留結構資訊
- 壓縮檔：自動解壓和批次處理
- 大檔案處理：支援 > 100MB 的文檔

### 3. 導入工具功能：
- Web UI 支援拖放上傳
- API 端點支援程式化導入
- 導入映射規則配置（欄位對應）
- 重複檢測（基於內容哈希）
- 批次導入支援（最多 1000 個檔案）

### 4. 向量化處理：
- 自動文檔分塊（每塊 500-1000 tokens）
- 重疊策略避免上下文丟失
- 批次向量化 API 調用優化成本
- 向量索引使用 HNSW 算法
- 處理失敗重試機制

### 5. 進度追蹤：
- 即時進度 WebSocket 更新
- 詳細的錯誤報告和日誌
- 可暫停/恢復的導入任務
- Email 通知完成或失敗

### 6. 元數據提取：
- 自動提取文檔屬性（作者、日期、關鍵字）
- 智能分類建議（基於內容）
- 實體識別（產品名、客戶名）
- 自動生成摘要

## 技術債務考量
- 考慮未來支援影片和音訊內容
- 預留多語言文檔處理能力
- 大規模時可能需要專門的向量數據庫
